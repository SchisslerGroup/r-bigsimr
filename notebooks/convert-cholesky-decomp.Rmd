---
title: "Conversion of Cholesky"
author: "Alex Knudson"
date: "6/12/2020"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      cache = TRUE)

library(magrittr)
```

condition number covariance estimation

# Idea

Given a Type $A$ correlation matrix $R_A$, we convert to a Pearson correlation $R_P$ matrix via $R_P = F_{A\rightarrow P}(R_A)$. Then with $R_P$, we compute the nearest positive semidefinite matrix $R_P^*$.

The rest of the algorithm depends on random multivariate normal samples with $R_P^*$. This can either be done internally with a call to a function such as `mvnfast::rmvn()` or via multiplication of independent standard normal samples $Z$ with the cholesky decomposition of $R_P^*$. To summarize, given a correlation matrix with $d$ columns, we wish to generate $n$ samples from a multivariate normal distribution:

$$
\begin{align*}
R_A &= \text{Corr}_A(X) \\
R_P &= F_{A\rightarrow P}(R_A) \\
R_P^* &= \text{NearestPosDef}(R_P) \\
L_P^* &= \text{Cholesky}(R_P^*) \\
Z_{[n\times d]} &\sim \mathcal{N}(0, 1) \\
\text{MVN} &\sim Z \times L_P^*
\end{align*}
$$

**Simple Example of the Concept**

```{r}
n <- 1e5
d <- 4

# Pearson Correlation matrix
R <- matrix(0.5, d, d)
diag(R) <- 1.0

# Cholesky decomposition
# Note that chol() returns the upper triangular factor
L_ <- chol(R)
L <- t(L_)
```


```{r, echo=FALSE}
cat("L*L_ == R:\t", all.equal(L %*% L_, R), "\n\n")
```


```{r}
Z <- matrix(rnorm(n*d), n, d)
M <- Z %*% L_
```


```{r, results='hold', echo=FALSE}
cat("cor(M):\n")
cor(M) %>% round(3)
```

## Modification of the Algorithm

Instead of converting the correlation matrix and then computing the nearest positive definite matrix, we are interested in seeing if we can compute the cholesky decomposition of $R_A$ and then running the decomposition through the conversion:

$$
\begin{align*}
R_A &= \text{Corr}_A(X) \\
L_A^* &= \text{Cholesky}(R_A) \\
L_P^* &= F_{A\rightarrow P}(L_A^*) \\
Z_{[n\times d]} &\sim \mathcal{N}(0, 1) \\
\text{MVN} &\sim Z \times L_P^*
\end{align*}
$$

## The Conversion Formulae


```{r}
convertCor <- function(rho, 
                       from = c("pearson", "spearman", "kendall"),
                       to = c("pearson", "spearman", "kendall"),
                       computeNearestPD = FALSE) {

  key <- paste(from, to)

  A <- switch (key,
               "pearson spearman" = function(r) (6/pi)*asin(r/2),
               "pearson kendall"  = function(r) (2/pi)*asin(r),
               "spearman pearson" = function(r) 2*sin(r*pi/6),
               "spearman kendall" = function(r) (2/pi)*asin(2*sin(r*pi/6)),
               "kendall pearson"  = function(r) sin(r*pi/2),
               "kendal spearman"  = function(r) (6/pi)*asin(sin(r*pi/2)/2),
               function(r) r
  )

  if (computeNearestPD) {
    return(Matrix::nearPD(A(rho), ensureSymmetry=FALSE, corr=TRUE)$mat)
  } else {
    return(A(rho))
  }
  
}
```


We can show that the above conversions work on Cholesky factors. We have already showed that $LL^* = R$

```{r}
L %*% L_
```

We want to show that 

$$
F_{A\rightarrow B}(L_A) \times F_{A\rightarrow B}(L_A^*) \approx R_B
$$

**Pearson to spearman**

```{r}
Rs <- convertCor(R, from = "pearson", to = "spearman")
Ls <- convertCor(L, from = "pearson", to = "spearman")
Ls_ <- t(Ls)
```


```{r}
round(Rs, 3)
round(Ls %*% Ls_, 3)
all.equal(round(Rs, 3), round(Ls %*% Ls_, 3))
```

**Pearson to Kendall**

```{r}
Rk <- convertCor(R, from = "pearson", to = "kendall")
Lk <- convertCor(L, from = "pearson", to = "kendall")
Lk_ <- t(Lk)
```


```{r}
round(Rk, 3)
round(Lk %*% Lk_, 3)
all.equal(round(Rk, 3), round(Lk %*% Lk_, 3))
```
It seems as though $P \rightarrow S$ conversion yields closer results than $P \rightarrow K$. We still need to test a few other conversions, namely $S \rightarrow P$ and $K \rightarrow P$. These next two will be more in line with how the algorithm is expected to be used.

**Spearman to Pearson**

```{r}
Rsp <- convertCor(R, from = "spearman", to = "pearson")
Lsp <- convertCor(L, from = "spearman", to = "pearson")
Lsp_ <- t(Lsp)
```


```{r}
round(Rsp, 3)
round(Lsp %*% Lsp_, 3)
all.equal(Rsp, Lsp %*% Lsp_)
```

This direction results in a symmetric matrix that **does not** have ones on the diagonal, meaning that it **is not** a valid correlation matrix. We can do one of three things: 1) Ignore the problem and use the Cholesky factor anyway, 2) multiply and set the diagonals to 1 and then refactor, or 3) multiply and use `cov2cor()` to ensure that it is a valid correlation matrix, then refactor.

```{r}
# Method 1
Msp1 <- bigsimr::rmvn(n, rep(0, d), Lsp_, isChol = TRUE)
(C1 <- bigsimr::fastCor(Msp1, method = "spearman")) %>% round(3)
```

```{r}
# Method 2
Rp2 <- Lsp %*% Lsp_
diag(Rp2) <- 1
Msp2 <- bigsimr::rmvn(n, rep(0, d), Rp2)
(C2 <- bigsimr::fastCor(Msp2, method = "spearman")) %>% round(3)
```

```{r}
# Method 3
Rp3 <- Lsp %*% Lsp_
Rp3 <- cov2cor(Rp3)
Msp3 <- bigsimr::rmvn(n, rep(0, d), Rp3)
(C3 <- bigsimr::fastCor(Msp3, method = "spearman")) %>% round(3)
```

```{r}
# Baseline
Rp0 <- convertCor(R, "spearman", "pearson", computeNearestPD = TRUE)
Msp0 <- bigsimr::rmvn(n, rep(0, d), Rp0)
(C0 <- bigsimr::fastCor(Msp0, method = "spearman")) %>% round(3)
```


```{r, results='hold'}
all.equal(C0, R)
all.equal(C1, R)
all.equal(C2, R)
all.equal(C3, R)
```

This idea seems to be okay for 

**Kendall to Pearson**

```{r}
Rkp <- convertCor(R, from = "kendall", to = "pearson")
Lkp <- convertCor(L, from = "kendall", to = "pearson")
Lkp_ <- t(Lkp)
```


```{r}
round(Rkp, 3)
round(Lkp %*% Lkp_, 3)
all.equal(Rkp, Lkp %*% Lkp_)
```

```{r}
# Method 1
Mkp1 <- bigsimr::rmvn(n, rep(0, d), Lkp_, isChol = TRUE)
(C1 <- bigsimr::fastCor(Mkp1, method = "kendall")) %>% round(3)
```

```{r}
# Method 2
Rp2 <- Lkp %*% Lkp_
diag(Rp2) <- 1
Mkp2 <- bigsimr::rmvn(n, rep(0, d), Rp2)
(C2 <- bigsimr::fastCor(Mkp2, method = "kendall")) %>% round(3)
```

```{r}
# Method 3
Rp3 <- Lkp %*% Lkp_
Rp3 <- cov2cor(Rp3)
Mkp3 <- bigsimr::rmvn(n, rep(0, d), Rp3)
(C3 <- bigsimr::fastCor(Mkp3, method = "kendall")) %>% round(3)
```

```{r}
# Baseline
Rp0 <- convertCor(R, "kendall", "pearson", computeNearestPD = TRUE)
Mkp0 <- bigsimr::rmvn(n, rep(0, d), Rp0)
(C0 <- bigsimr::fastCor(Msp0, method = "kendall")) %>% round(3)
```

```{r, results='hold'}
all.equal(C0, R, check.attributes = FALSE)
all.equal(C1, R, check.attributes = FALSE)
all.equal(C2, R, check.attributes = FALSE)
all.equal(C3, R, check.attributes = FALSE)
```

For Kendall to Pearson, methods 1, 2, and 3 offer similar performance that is even better than the baseline method.

Let's see how this scales with the number of dimensions.

## Larger Dimensions

```{r}
n <- 1e5
d <- 100

# Pearson Correlation matrix
R <- matrix(0.5, d, d)
diag(R) <- 1.0

L_ <- chol(R)
L <- t(L_)
```

**Spearman to Pearson**

```{r}
Rsp <- convertCor(R, from = "spearman", to = "pearson")
Lsp <- convertCor(L, from = "spearman", to = "pearson")
Lsp_ <- t(Lsp)


# Method 1
Msp1 <- bigsimr::rmvn(n, rep(0, d), Lsp_, isChol = TRUE)
C1 <- bigsimr::fastCor(Msp1, method = "spearman")

# Method 2
Rp2 <- Lsp %*% Lsp_
diag(Rp2) <- 1
Msp2 <- bigsimr::rmvn(n, rep(0, d), Rp2)
C2 <- bigsimr::fastCor(Msp2, method = "spearman")

# Method 3
Rp3 <- Lsp %*% Lsp_
Rp3 <- cov2cor(Rp3)
Msp3 <- bigsimr::rmvn(n, rep(0, d), Rp3)
C3 <- bigsimr::fastCor(Msp3, method = "spearman")

# Baseline
Rp0 <- convertCor(R, "spearman", "pearson", computeNearestPD = TRUE)
Msp0 <- bigsimr::rmvn(n, rep(0, d), Rp0)
C0 <- bigsimr::fastCor(Msp0, method = "spearman")
```


```{r, results='hold'}
all.equal(C0, R)
all.equal(C1, R)
all.equal(C2, R)
all.equal(C3, R)
```
The baseline method is still the best, but methods 1 and 3 are not bad. Method 2 (setting the diagonal to 1) seems to be more detrimental.

**Kendall to Pearson**

```{r, error=TRUE}
Rsp <- convertCor(R, from = "kendall", to = "pearson")
Lsp <- convertCor(L, from = "kendall", to = "pearson")
Lsp_ <- t(Lsp)


# Method 1
Msp1 <- bigsimr::rmvn(n, rep(0, d), Lsp_, isChol = TRUE)
C1 <- bigsimr::fastCor(Msp1, method = "kendall")

# Method 2
Rp2 <- Lsp %*% Lsp_
diag(Rp2) <- 1
Msp2 <- bigsimr::rmvn(n, rep(0, d), Rp2)
C2 <- bigsimr::fastCor(Msp2, method = "kendall")

# Method 3
Rp3 <- Lsp %*% Lsp_
Rp3 <- cov2cor(Rp3)
Msp3 <- bigsimr::rmvn(n, rep(0, d), Rp3)
C3 <- bigsimr::fastCor(Msp3, method = "kendall")

# Baseline
Rp0 <- convertCor(R, "kendall", "pearson", computeNearestPD = TRUE)
Msp0 <- bigsimr::rmvn(n, rep(0, d), Rp0)
C0 <- bigsimr::fastCor(Msp0, method = "kendall")
```


```{r, results='hold'}
all.equal(C0, R, check.attributes = FALSE)
all.equal(C1, R, check.attributes = FALSE)
# all.equal(C2, R, check.attributes = FALSE) # Fails
all.equal(C3, R, check.attributes = FALSE)
```


For Kendall, the baseline method far outperforms methods 1 and 3. Method 2 fails by resulting in a non-positive definite correlation matrix.

# Real Data

We are now going to take real data and apply the methods described above. The data is the breast cancer data set filtered to use just the top 99% expressing genes (206 columns).

```{r}
# Load in the data
dat <- local({
  library(tidyverse)
  
  q <- 0.999
  
  dat0 <- readRDS(
    file = "~/Downloads/complete_processed_tcga2stat_RNASeq2_with_clinical.rds")
  
  # Select just the BRCA genes
  brca0 <- dat0 %>%
    filter(disease == "BRCA") %>%
    select(-c("patient":"tumorsize"))
  
  brca1 <- round(brca0, 0)
  
  # Subset the BRCA data to the cutpoint
  brca_median <- apply(brca1, 2, median)
  cut_point <- quantile(brca_median, q)
  keep_genes <- names(brca_median)[brca_median >= cut_point]
  (d <- length(keep_genes))
  
  brca1 %>%
    select(all_of(keep_genes))
})

mom_nbinom <- function(x) {
  m <- mean(x)
  s <- sd(x)
  rlang::call2("qnbinom", size = m^2 / (s^2 - m), prob = m / s^2)
}

margins <- apply(dat, 2, mom_nbinom)
```

## Spearman

### Normal Method with Inverse CDF Step

$$
\begin{align*}
R_A &= \text{Corr}_A(X) \\
R_P &= F_{A\rightarrow P}(R_A) \\
R_P^* &= \text{NearestPosDef}(R_P) \\
L_P^* &= \text{Cholesky}(R_P^*) \\
Z_{[n\times d]} &\sim \mathcal{N}(0, 1) \\
\text{M} &= Z \times L_P^* \\
U &= \Phi(M) \\
X_i &= G_i^{-1}(U_i) \quad \text{for } i = 1\ldots d
\end{align*}
$$


```{r}
n <- 1e5
d <- ncol(dat)

Rs  <- bigsimr::fastCor(dat, method = "spearman")
Rp  <- convertCor(Rs, "spearman", "pearson")
Rp  <- Matrix::nearPD(Rp, corr = TRUE, ensureSymmetry = FALSE)$mat
Lp_ <- chol(Rp)

X <- local({
  Z   <- matrix(rnorm(n * d), n, d)
  M   <- Z %*% Lp_
  U   <- pnorm(M)
  
  u2m <- function(u, margin) {
    margin$p <- quote(u)
    eval(margin)
  }
  
  `%dopar%` <- foreach::`%dopar%`
  cl <- parallel::makeCluster(parallel::detectCores(), type = "FORK")
  doParallel::registerDoParallel(cl)
  
  rv <- foreach::foreach(i = 1:d, .combine = 'cbind') %dopar% {
    u2m(U[, i], margins[[i]])
  }
  
  parallel::stopCluster(cl)
  
  rv
})
```


```{r}
Rs_hat <- bigsimr::fastCor(X, method = "spearman")
all.equal(Rs, Rs_hat, check.attributes = FALSE)
```


### Modified Method with Inverse CDF Step

$$
\begin{align*}
R_A &= \text{Corr}_A(X) \\
L_A^* &= \text{Cholesky}(R_A) \\
L_P^* &= F_{A\rightarrow P}(L_A^*) \\
Z_{[n\times d]} &\sim \mathcal{N}(0, 1) \\
\text{M} &\sim Z \times L_P^* \\
U &= \Phi(M) \\
X_{\cdot i} &= G_i^{-1}(U_i) \quad \text{for } i = 1\ldots d
\end{align*}
$$


```{r}
n <- 1e5
d <- ncol(dat)

Rs  <- bigsimr::fastCor(dat, method = "spearman")
Ls_ <- chol(Rs)
Lp_ <- convertCor(Ls_, "spearman", "pearson")

X1 <- local({
  Z   <- matrix(rnorm(n * d), n, d)
  M   <- Z %*% Lp_
  U   <- pnorm(M)
  
  u2m <- function(u, margin) {
    margin$p <- quote(u)
    eval(margin)
  }
  
  `%dopar%` <- foreach::`%dopar%`
  cl <- parallel::makeCluster(parallel::detectCores(), type = "FORK")
  doParallel::registerDoParallel(cl)
  
  rv <- foreach::foreach(i = 1:d, .combine = 'cbind') %dopar% {
    u2m(U[, i], margins[[i]])
  }
  
  parallel::stopCluster(cl)
  
  rv
})
```


```{r}
Rs_hat1 <- bigsimr::fastCor(X1, method = "spearman")
all.equal(Rs, Rs_hat1, check.attributes = FALSE)
```
